{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First assume every page contains only structured texts with one column \n",
    "\n",
    "tables = []\n",
    "text_blocks = []\n",
    "\n",
    "with pdfplumber.open(\"data/raw/IFRS_9.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text_blocks.append(page.extract_text())\n",
    "        tables.append(page.extract_tables()) #Will help identify page with table to perform specific cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff976228",
   "metadata": {},
   "source": [
    "### First preprocessing : entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip the introduction and delete the appendix of amendments dates / lists of participants\n",
    "#text_blocks = text_blocks[5:-10]\n",
    "#tables = tables[5:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_text(text_blocks) :\n",
    "# First, header and footpage\n",
    "    text_blocks = [re.sub(\"IFRS 9\\n\", \"\", block) for block in text_blocks]\n",
    "    text_blocks = [re.sub(r\"\\n.*© IFRS Foundation.*\", \"\", block) for block in text_blocks]\n",
    "\n",
    "#Paragraph \n",
    "    text_blocks = [re.sub(r\"(Chapter .*)\", r\"\\n\\1\\n\",block) for block in text_blocks] #Add an extra space before and after Chapter\n",
    "\n",
    "#    text_blocks = [re.sub(r\"(?<![a-z])([A-Z]?\\d\\.\\d\\.\\d )\", r\"\\n\\1\",block) for block in text_blocks] #Add an extra space before a subpart\n",
    "\n",
    "#    text_blocks = [re.sub(r\"(?<![a-z])(\\n[A-Z]?\\d\\.\\d )\", r\"\\n\\1\",block) for block in text_blocks] #Add an extra space before a subpart\n",
    "#    text_blocks = [re.sub(r\"(?<![a-z])(\\n[A-Z]\\d\\.\\d\\.\\d )\", r\"\\n\\1\",block) for block in text_blocks] #Add an extra space before a subpart\n",
    "\n",
    "# A paragraph is divided in multiple lines so we need to fuse it back into one line\n",
    "    text_blocks = [re.sub(r\"([a-z,A-Z,\\(\\)])\\n([a-z,A-Z])\", r\"\\1 \\2\",block) for block in text_blocks]\n",
    "\n",
    "# Sometimes sub sections (X.X.Y) aren't properly spaced form the previous line\n",
    "    text_blocks = [re.sub(r\" (\\d\\.\\d\\.\\d [A-Z])\", r\"\\n\\1\",block) for block in text_blocks]\n",
    "\n",
    "#Specific cases\n",
    "    text_blocks = [re.sub(r\"([a-z])\\n([a-z,1-9])\", r\"\\1 \\2\", block) for block in text_blocks] #Avoid that (.) subpart be fuse into previous line\n",
    "    text_blocks = [re.sub(r\"([1-9])\\n([a-z])\", r\"\\1 \\2\", block) for block in text_blocks] #If not this specific, new subpart can be added into previous line\n",
    "    text_blocks = [re.sub(r\"(:)\\n([A-Z])\", r\"\\1 \\2\", block) for block in text_blocks]\n",
    "\n",
    "    #If new page has a sentence of previous page last paragraph, fuse it to the paragraph\n",
    "    for i in range(1,len(text_blocks)-1) :\n",
    "    \n",
    "        if text_blocks[i] : #If page isn't empty (last page IFRS)\n",
    "        \n",
    "            if text_blocks[i][0].islower() and text_blocks[i-1][-1].islower() :\n",
    "        \n",
    "                end_sentence = text_blocks[i].split(\"\\n\")[0]\n",
    "                new_block = \"\\n\".join(text_blocks[i].split(\"\\n\")[1:])\n",
    "        \n",
    "                text_blocks[i] = new_block\n",
    "                text_blocks[i-1] = text_blocks[i-1]+\" \"+end_sentence\n",
    "    \n",
    "    return text_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16036a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blocks = clean_all_text(text_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c7669",
   "metadata": {},
   "source": [
    "### Second preprocessing : definitions pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca53217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_words_into_entries(words, y_tol=3, line_spacing_tol=12):\n",
    "    # Sort by vertical, then horizontal\n",
    "    words = sorted(words, key=lambda w: (w[1], w[0]))\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_y = None\n",
    "    last_line_y = None\n",
    "\n",
    "    for w in words:\n",
    "        x0, y0, x1, y1, text, block, line, span = w\n",
    "\n",
    "        # Case 1 — first word → start entry\n",
    "        if current_y is None:\n",
    "            current_y = y0\n",
    "            last_line_y = y0\n",
    "            current_entry.append(w)\n",
    "            continue\n",
    "\n",
    "        # Case 2 — same line (y difference small)\n",
    "        if abs(y0 - last_line_y) <= y_tol:\n",
    "            current_entry.append(w)\n",
    "            continue\n",
    "\n",
    "        # Case 3 — next line but should be merged (definition continuation)\n",
    "        if abs(y0 - last_line_y) <= line_spacing_tol:\n",
    "            current_entry.append(w)\n",
    "            last_line_y = y0\n",
    "            continue\n",
    "\n",
    "        # Case 4 — too far → new entry\n",
    "        entries.append(current_entry)\n",
    "        current_entry = [w]\n",
    "        current_y = y0\n",
    "        last_line_y = y0\n",
    "\n",
    "    # push last one\n",
    "    if current_entry:\n",
    "        entries.append(current_entry)\n",
    "\n",
    "    return entries\n",
    "\n",
    "def entry_to_text(entry, type):\n",
    "\n",
    "    entry = sorted(entry, key=lambda w: (w[6],w[0]))\n",
    "    text = \" \".join(w[4] for w in entry)\n",
    "    \n",
    "    if type == \"col\" :\n",
    "        return [text, entry[0][1]]\n",
    "    else :\n",
    "        return text\n",
    "    \n",
    "def normalize_def(term):\n",
    "    import re\n",
    "    from unidecode import unidecode\n",
    "\n",
    "    t = term.lower()\n",
    "    t = unidecode(t)\n",
    "    t = re.sub(r\"[^a-z0-9]+\", \"_\", t)\n",
    "    t = re.sub(r\"_+\", \"_\", t).strip(\"_\")\n",
    "    return t+\"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e316a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_if_multi_col(doc, page_num):\n",
    "\n",
    "    page = doc[page_num]\n",
    "    words = page.get_text_words()\n",
    "    if not words:\n",
    "        return \n",
    "\n",
    "    # keep python list\n",
    "    raw = page.get_text_words()\n",
    "\n",
    "    # extract columns with list comprehension\n",
    "    x0 = np.array([w[0] for w in raw], dtype=float)\n",
    "    y0 = np.array([w[1] for w in raw], dtype=float)\n",
    "    texts = [w[4] for w in raw]\n",
    "\n",
    "    words = raw  # keep original structure\n",
    "\n",
    "    # Column part\n",
    "    width = page.rect.width\n",
    "    bins_x = np.linspace(0, width, 200)\n",
    "    density_x, _ = np.histogram(x0, bins=bins_x)\n",
    "\n",
    "    idx_left = np.argmax(density_x > 4)\n",
    "    if density_x[idx_left] == 0: #If no \n",
    "        return \n",
    "\n",
    "    big_peaks = np.where(density_x > 20)[0]\n",
    "    if len(big_peaks) != 1: #If cols > 2 \n",
    "        return \n",
    "\n",
    "    idx_sep = big_peaks[0]\n",
    "\n",
    "    # Check init distance between peaks\n",
    "    if abs(idx_left - idx_sep) <= 20:\n",
    "        return \n",
    "\n",
    "    midpoint_x = bins_x[idx_sep - 1]\n",
    "\n",
    "    # Row part\n",
    "    height = page.rect.height\n",
    "    bins_y = np.linspace(0, height, 200)\n",
    "    density_y, _ = np.histogram(y0, bins=bins_y)\n",
    "\n",
    "    row_peaks = np.where(density_y > 15)[0]\n",
    "\n",
    "    # If no row separator is detected → everything is 2-column\n",
    "    if len(row_peaks) == 0:\n",
    "        cutoff_y = height\n",
    "    else:\n",
    "        cutoff_y = bins_y[row_peaks[0] - 1]\n",
    "\n",
    "    left_words  = [w for w in words if (w[0] < midpoint_x and w[1] < cutoff_y)]\n",
    "    right_words = [w for w in words if (w[0] >= midpoint_x and w[1] < cutoff_y)]\n",
    "    below_words = [w for w in words if w[1] >= cutoff_y]\n",
    "\n",
    "\n",
    "    left_entries  = group_words_into_entries(left_words)\n",
    "    right_entries = group_words_into_entries(right_words)\n",
    "    below_entries = group_words_into_entries(below_words)\n",
    "\n",
    "    left_texts  = [entry_to_text(e, \"col\") for e in left_entries]\n",
    "    right_texts = [entry_to_text(e, \"col\") for e in right_entries]\n",
    "    below_texts = [entry_to_text(e, \"row\") for e in below_entries]\n",
    "\n",
    "    final_te = []\n",
    "    for rtxt, ry in right_texts:\n",
    "        merged = False\n",
    "        for ltxt, ly in left_texts:\n",
    "            if abs(ry - ly) < 1:\n",
    "                final_te.append(normalize_def(ltxt) + \" \" + rtxt)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            final_te.append(rtxt)\n",
    "\n",
    "    final_te.extend(t for t in below_texts)\n",
    "\n",
    "    return \"\\n\".join(final_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(\"/home/hlm/Documents/Mini-RAG/data/raw/IFRS_9.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a362a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(63,68) : \n",
    "    text_blocks[i] = detect_if_multi_col(doc,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443308a",
   "metadata": {},
   "source": [
    "### Third preprocessing : table pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know when to use OCR\n",
    "ocr_needed = []\n",
    "tables_ = []\n",
    "\n",
    "for page in range(len(doc)) :\n",
    "    if len(doc[page].get_drawings()) > 5 : \n",
    "\n",
    "        if not tables[page] : #Tables not recognized by pdfplumber\n",
    "            ocr_needed.append(page)\n",
    "        else :\n",
    "            tables_.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c1747",
   "metadata": {},
   "source": [
    "Identified tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(headers, cols, continued = False):\n",
    "    md = \"\"\n",
    "\n",
    "    if not continued :\n",
    "        for i in headers :\n",
    "            md += f\"| {i} \"\n",
    "        md+=\"|\\n\"\n",
    "\n",
    "        for i in headers :\n",
    "            md += \"|---\"\n",
    "        md+=\"|\\n\"\n",
    "\n",
    "    for i in cols :\n",
    "        i = re.sub(r\"([a-z])\\n([a-z])\", r\"\\1 \\2\", i)\n",
    "        i = re.sub(r\"-\\n([a-z])\", r\"\\1\", i)\n",
    "        i = i.replace(\"\\n\", \"<br>\")\n",
    "\n",
    "        md += f\"| {i} \"\n",
    "\n",
    "    return md\n",
    "\n",
    "def transform_table_to_use(tables_, text_blocks) :\n",
    "\n",
    "    for num in tables_ :\n",
    "    \n",
    "        table = pdf.pages[num].find_table().cells \n",
    "    \n",
    "        y_min = 10000\n",
    "        y_max = 0\n",
    "        for elem in table :\n",
    "            y_min = min(elem[1],y_min)\n",
    "            y_max = max(elem[3],y_max)\n",
    "\n",
    "        before =[]\n",
    "        after = []\n",
    "        \n",
    "        for i in pdf.pages[num].extract_words() :\n",
    "            if i[\"top\"] < y_min and len(before) < 10:\n",
    "                before.append(i[\"text\"])\n",
    "            if i[\"bottom\"] > y_max and len(after) < 10 :\n",
    "                after.append(i[\"text\"])\n",
    "\n",
    "        #Reconstruct the true page\n",
    "        new_page = \"\"\n",
    "        tablemm = tables[num][0]\n",
    "        headers = tablemm[0]\n",
    "        cols = tablemm[1]\n",
    "        \n",
    "        if len(after) > 5 :\n",
    "            past_table = False\n",
    "\n",
    "            after_text = \" \".join(after[:5])\n",
    "            text = text_blocks[num]\n",
    "\n",
    "            for j in text.split(\"\\n\") :\n",
    "                if past_table :\n",
    "                    new_page += j\n",
    "\n",
    "                elif after_text in j :\n",
    "                    past_table = True\n",
    "                    new_page += after_text+j.split(after_text)[1]\n",
    "        \n",
    "        else : \n",
    "            if \"...continued\" in before :\n",
    "                new_page += to_markdown(headers, cols, continued=True)\n",
    "            else :\n",
    "                new_page += to_markdown(headers,cols)\n",
    "\n",
    "        text_blocks[num] = new_page\n",
    "\n",
    "    return text_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc116315",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blocks = transform_table_to_use(tables_, text_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65827fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d4b3c",
   "metadata": {},
   "source": [
    "The first page that needs ocr will be deleted after and only the 73 really needs ocr. I actually used ChatGPT to get direcctly the text from the page directly and the layout as it needed really specific instructions to get good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = ['''IFRS 9\\n\n",
    "Derecognition of financial assets (Section 3.2)\\n\n",
    "\\n\n",
    "B3.2.1\\n\n",
    "The following flow chart illustrates the evaluation of whether and to what extent a financial asset is derecognised.\\n\n",
    "\\n\n",
    "Consolidate all subsidiaries [Paragraph 3.2.1]\\n\n",
    "\\n\n",
    "Determine whether the derecognition principles below are applied to a part or all of an asset (or group of similar assets) [Paragraph 3.2.2]\\n\n",
    "\\n\n",
    "Have the rights to the cash flows from the asset expired? [Paragraph 3.2.3(a)]\\n\n",
    "Yes → Derecognise the asset\\n\n",
    "No → Continue\\n\n",
    "\\n\n",
    "Has the entity transferred its rights to receive the cash flows from the asset? [Paragraph 3.2.4(a)]\\n\n",
    "Yes → Continue\\n\n",
    "No → Continue\\n\n",
    "\\n\n",
    "Has the entity assumed an obligation to pay the cash flows from the asset that meets the conditions in paragraph 3.2.5? [Paragraph 3.2.4(b)]\\n\n",
    "No → Continue to recognise the asset\\n\n",
    "Yes → Continue\\n\n",
    "\\n\n",
    "Has the entity transferred substantially all risks and rewards? [Paragraph 3.2.6(a)]\\n\n",
    "Yes → Derecognise the asset\\n\n",
    "No → Continue\\n\n",
    "\\n\n",
    "Has the entity retained substantially all risks and rewards? [Paragraph 3.2.6(b)]\\n\n",
    "Yes → Continue to recognise the asset\\n\n",
    "No → Continue\\n\n",
    "\\n\n",
    "Has the entity retained control of the asset? [Paragraph 3.2.6(c)]\\n\n",
    "No → Derecognise the asset\\n\n",
    "Yes → Continue to recognise the asset to the extent of the entity’s continuing involvement\\n\n",
    "\\n\n",
    "A436\\n\n",
    "© IFRS Foundation\\n\n",
    "''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = \"\".join(layout)\n",
    "text_blocks[73] = layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c387499",
   "metadata": {},
   "source": [
    "Get rid of unecessary pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8986c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip the introduction and delete the appendix of amendments dates / lists of participants\n",
    "text_blocks = text_blocks[5:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blocks = clean_all_text(text_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daccf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = \"\".join(text_blocks)\n",
    "cc = re.sub(r\"\\n{2,}\",\"\\n\", cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a0993",
   "metadata": {},
   "source": [
    "Nearly finished preprocess, need to found a solution for a slight problem on chapter acquisition during multi col"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
